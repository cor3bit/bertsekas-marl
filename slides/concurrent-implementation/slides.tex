\documentclass{beamer}

\mode<presentation> {


\usetheme{Singapore}
}


\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{hyperref}

\usepackage{graphicx} % Allows including images
\graphicspath{ {images/} }
\usepackage{xcolor}




\AtBeginSection[]{
    \begin{frame}
        \vfill
        \centering
        \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
            \usebeamerfont{title}\insertsectionhead\par%
        \end{beamercolorbox}
        \vfill
    \end{frame}
}


%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[MARL]{Concurrent Implementation 
	of the 
	Multiagent Rollout Algorithm}
\author{Mikalai Korbit}

\institute[IMT]
{
    IMT School for Advanced Studies Lucca 
}
\date{\today} 


\begin{document}

    \begin{frame}
        \titlepage 
    \end{frame}

    \begin{frame}
        \frametitle{Outline}
        \tableofcontents 
    \end{frame}


%----------------------------------------------------------------------------------------
%	PRESENTATION SLIDES
%----------------------------------------------------------------------------------------


%------------------------------------------------


    \section{MARL Overview}
%------------------------------------------------




    \begin{frame}
        \frametitle{Motivation}

        \begin{block}{Multi-agent systems are ubiquitous}
            Eg. fleet of drones, factory robots, self-driving cars.
        \end{block}

        \begin{block}{Recent advances in RL applications}
            Eg. AlphaGo/AlphaZero, playing Starcraft, robotic control.
        \end{block}

        \begin{block}{Utilize modern computer architecture and software frameworks}
            Eg. cloud computing, stacks of graphics cards, TPUs;
            PyTorch, OpenAI gyms.
        \end{block}

        \begin{block}{Benefits of modeling a problem as MARL}
            Scalability, robustness, faster learning through experience sharing,
            parallel computation.
        \end{block}

    \end{frame}


%------------------------------------------------


    \begin{frame}
        \frametitle{Multi-Agent Reinforcement Learning Problem}
        Inherits Reinforcement Learning characteristics:
        \begin{itemize}
            \item Learning how to map situations into actions
            \item Trial-and-error search
            \item Delayed feedback
            \item Trade-off between exploration and exploitation
            \item Sequential decision making
            \item Agent's actions affect the subsequent data it receives
        \end{itemize}

        Adds multi-agent features:
        \begin{itemize}
            \item Actions of one agent influence other agents' rewards
            \item Communication problem
            \item Fully cooperative, fully info sharing (DP) vs. partial info sharing
            \item Curse of dimensionality (more severe than in RL)
        \end{itemize}

    \end{frame}



%------------------------------------------------


    \begin{frame}
        \frametitle{Multi-Agent MDP}

        \begin{figure}
            \includegraphics[scale=0.65]{1a_marl}
            \caption{MARL Problem. Source: Sadhu, Konar (2020)}
        \end{figure}

        \begin{itemize}
            \item All agents see the global state $s$
            \item Individual actions: $u^{a} \in U$
            \item State transitions: $P\left(s^{\prime} \mid s, \mathbf{u}\right): S \times \mathbf{U} \times S \rightarrow[0,1]$
            \item Shared team reward: $S \times \mathbf{U} \rightarrow \mathbb{R}$


        \end{itemize}

    \end{frame}







%------------------------------------------------
    \section{Concurrency in Python}
%------------------------------------------------

\begin{frame}

	\frametitle{Concurrency in One Slide}
	
	\begin{figure}
		\includegraphics[scale=0.35]{2a_mindmap}
		\caption{Concurrency Mindmap by Luciano Ramalho (2019)}
	\end{figure}
	
\end{frame}

%------------------------------------------------

\begin{frame}
	
	\frametitle{Concurrency Tools in Python}
    
    \begin{itemize}
		\item TODO
		\item TODO
		\item TODO
	\end{itemize}

\end{frame}









%------------------------------------------------
\section{Parallel Implementaion}
%------------------------------------------------



    \begin{frame}
        \frametitle{Key Ideas}


        \begin{block}{Deal with the exponential increase in the action space}
            $\rightarrow$ Introduce a form of sequential
            agent-by-agent one-step lookahead minimization --
            \textit{multiagent rollout}

        \end{block}

        \begin{block}{Compute the agent actions in parallel}
            $\rightarrow$ Decouple sequential
            agent-by-agent computation with \textit{precomputed
            signaling policy} that embodies agent coordination
        \end{block}


    \end{frame}


%------------------------------------------------



    \begin{frame}
        \frametitle{The Setting}

        \begin{itemize}
            \item $P2_F$ -- stochastic discrete-time optimal control problem over a finite horizon, with perfect information on the state
            \item Fully cooperative
            \item Tested in Spiders-And-Flies environment
        \end{itemize}

        $$x_{k+1}=f_{k}\left(x_{k}, u_{k}, w_{k}\right), \quad k=0,1, \ldots, N-1$$

        $$J_{\pi}\left(x_{0}\right)=E\left\{g_{N}\left(x_{N}\right)+\sum_{k=0}^{N-1} g_{k}\left(x_{k}, \mu_{k}\left(x_{k}\right), w_{k}\right)\right\}$$


    \end{frame}


%------------------------------------------------


    \begin{frame}
        \frametitle{Policy Iteration and Rollout}

        \begin{figure}
            \includegraphics[scale=0.65]{2a_pi}
            \caption{Policy Iteration Algorithm. Source: Bertsekas (2020)}
        \end{figure}


        \begin{itemize}
            \item Fundamental property: policy improvement
            $$J_{k, \tilde{\pi}}\left(x_{k}\right) \leq J_{k, \pi}\left(x_{k}\right), \quad \forall x_{k}, k$$

        \end{itemize}

    \end{frame}

%------------------------------------------------

    \begin{frame}
        \frametitle{Standard Rollout Algorithm}

        \begin{itemize}
            \item Rollout is one-time policy iteration
            \item Start with the initial
            state $x0$, and generate a trajectory:

            $$
            \left\{x_{0}, \tilde{u}_{0}, x_{1}, \tilde{u}_{1}, \ldots, x_{N-1}, \tilde{u}_{N-1}, x_{N}\right\}
            $$

            Where  $\tilde{u}_{k}$ is

            $$
            \begin{aligned}
                \tilde{u}_{k} \in \arg \min _{u_{k} \in U_{k}\left(x_{k}\right)} E\left\{g_{k}\left(x_{k}, u_{k}, w_{k}\right) + \right.\\
                &\left.+J_{k+1, \pi}\left(f_{k}\left(x_{k}, u_{k}, w_{k}\right)\right)\right\}
            \end{aligned}
            $$

            \item Defines rollout policy that
            possesses \textbf{cost improvement property}
            and \textbf{robustness property} (can adapt to
            changes in data distributions online)

            \item Works when argmin over small set of $U$!


        \end{itemize}


    \end{frame}


%------------------------------------------------


    \begin{frame}
        \frametitle{Standard Rollout for MA Case (All-at-once Rollout)}

        \begin{itemize}
            \item The control constraint set becomes
            the Cartesian product

            $$
            U_{k}\left(x_{k}\right)=U_{k}^{1}\left(x_{k}\right) \times \cdots \times U_{k}^{m}\left(x_{k}\right)
            $$

            \item Argmin is now computed over $q^m$! (where $q$ is an upper bound to the number of
            controls in $U_{k}$, $m$ is the number of agents)

            \item Idea: trade-off control space complexity
            with state space complexity

        \end{itemize}

    \end{frame}

%------------------------------------------------





    \begin{frame}
        \frametitle{One-at-a-time Rollout (Multiagent Rollout)}

        \begin{figure}
            \includegraphics[scale=0.4]{2b}
            \caption{One-at-a-time action selection. Source: Bertsekas (2020)}
        \end{figure}


        \begin{enumerate}
            \item Break down $u_k$ into the sequence of $m$ actions:
            $u^1_k, u^2_k, ..., u^m_k$

            \item Introduce artificial states $\left(x_{k}, u_{k}^{1}\right),\left(x_{k}, u_{k}^{1}, u_{k}^{2}\right), \ldots,\left(x_{k}, u_{k}^{1}, \ldots, u_{k}^{m-1}\right)$
            \item $u^m_k$ marks the transition to the new state
            $x_{k+1}=f(x_k, u_k, w_k)$ incurring cost $g_k(x_k, u_k, w_k)$
        \end{enumerate}


    \end{frame}


%------------------------------------------------

    \begin{frame}
        \frametitle{Benefits of the Multiagent Rollout Algorithm}

        Past controls determined by
        the rollout policy, and the future controls determined by
        the base policy!



        \begin{itemize}
            \item Reducing the action space by increasing the state space. Reasonable since Q-factor minimization is performed for just
            one state at each stage.

            \item We reduce the computation complexity
            from $O(q^m)$ to $O(qm)$, $q=|U|$

            \item In addition to that, solves coordination.
            problem.

            \item Preserves \textbf{cost improvement property}
            (see Bertsekas, part II.D
            for proof by induction for $m=2$).

        \end{itemize}


    \end{frame}


%------------------------------------------------
    \begin{frame}
        \frametitle{Multiagent Rollout Assumptions}

        \begin{enumerate}
            \item All agents have access to the current state $x_k$;

            \item There is an order in which agents compute and apply
            their local controls;

            \item There is ``intercommunication'' between agents,
            so that agent $l$ knows the local controls
            $u_k^1, u_k^2, ..., u_k^{l-1}$
            computed by the
            predecessor agents    $1, 2, ..., l-1$
            in the given order.
        \end{enumerate}


    \end{frame}


%------------------------------------------------

%------------------------------------------------
    \begin{frame}

        \frametitle{Ordering of Agents}



        \begin{itemize}
            \item Instead of predefined or random order,
            at each step $k$ optimize over single
            agent's Q-factors.

            \item Simulate $m$ sequences where each agent
            acts first, select the one with minimal Q-factor,
            ``compete'' for the second place with $m-1$ agents,
            etc.

            \item Total number of minimizations:
            $$
            m+(m-1)+\cdots+1=\frac{m(m+1)}{2}
            $$


            \item Computations can be parallelized.
        \end{itemize}


    \end{frame}




    \begin{frame}
        \frametitle{Approximate Policy Iteration with Agent-by-Agent Policy Improvement}

        \begin{figure}
            \includegraphics[scale=0.55]{3a_api}
            \caption{Approximate Policy Iteration. Source: Bertsekas (2020)}
        \end{figure}


        \begin{itemize}
            \item Approximate policy improvement property: With approximations, policy improvement holds approximately
            \item If a single policy iteration is done (rollout), no need to train value and policy networks
            \item Multiple policy iterations can be done only with off-line training

        \end{itemize}
    \end{frame}


%------------------------------------------------


    \begin{frame}
        \frametitle{Parallel Computation of Agents' Controls}
        \begin{itemize}
            \item Reminder: parallel computation vs. action coordination.

            \item First Attempt: since the agent $l$
            does not know the
            rollout controls for the agents $1, ..., l âˆ’ 1$,
            uses
            the controls $\mu_{k}^{1}\left(x_{k}\right), \ldots, \mu_{k}^{\ell-1}\left(x_{k}\right)$ of the base policy in their
            place.

            \item Drawback: does not preserve cost improvement property.

        \end{itemize}
    \end{frame}


%------------------------------------------------

    \begin{frame}
        \frametitle{Autonomous Multiagent Rollout}
        \begin{itemize}
            \item Second Attempt: assume that once the agents know the
            state, they use precomputed approximations to the control
            components of the preceding agents, and compute their own
            control components in parallel and asynchronously --
            \textbf{autonomous multiagent rollout}.


            \item How to compute approximations? Train a neural network
            off-line training with training samples generated
            through the rollout policy -- \textbf{signaling policy}.


            \item Use base and signaling policies
            to generate a rollout policy
            $\tilde{\pi}=\left\{\tilde{\mu}_{0}, \ldots, \tilde{\mu}_{N-1}\right\}$
            autonomously in parallel.

        \end{itemize}
    \end{frame}


%------------------------------------------------



    \begin{frame}
        \frametitle{Synchronized Autonomous Multiagent Rollout}

        \begin{itemize}
            \item What if we allow periodic updates
            of the signaling policies?

        \end{itemize}


        \begin{figure}
            \includegraphics[scale=0.65]{3b_idea}
        \end{figure}


    \end{frame}








%------------------------------------------------
%------------------------------------------------
%------------------------------------------------


    \section{Conclusion}

    \begin{frame}
        \frametitle{Conclusion}
        \begin{itemize}
            \item MARL problems are especially prone to the
            curse of the dimensionality problem;

            \item We could reduce the action space
            by allowin agent-be-agent updates;

            \item We could parallelize computations
            by adding a signaling policy (precalculated offline);

            \item Multi-agent rollout can be extended with
            approximate policy iteration.


        \end{itemize}
    \end{frame}


    \begin{frame}
        \frametitle{References}
        \footnotesize{
            \begin{thebibliography}{10}

                \bibitem{bert20}\label{bert20}
                Dimitri Bertsekas -- Multiagent Reinforcement Learning: Rollout and Policy Iteration (2020). Web:
                \url{https://web.mit.edu/dimitrib/www/Multiagent_Sinica_2020.pdf}

                \bibitem{whiteson19}\label{whiteson20}
                Shimon Whiteson -- Multi-Agent Reinforcement Learning Reinforcement (July 2019) [Eastern European Machine Learning Summer School Seminar].


                \bibitem{sadhu20}\label{sadhu20}
                Arup Kumar Sadhu, Amit Konar --
                Multi-Agent Coordination,
                A Reinforcement Learning Approach (2020).


            \end{thebibliography}
        }
    \end{frame}

%------------------------------------------------

    \begin{frame}

        \begin{center}
            \Huge Thanks for
            \\
            your attention!
        \end{center}

    \end{frame}

%----------------------------------------------------------------------------------------

\end{document}